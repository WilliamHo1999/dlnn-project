{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2384cd9b-923a-427e-9204-7e05497e45cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import PIL.Image\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "from src.data.ImageNet100ClassesDataset import ImageNet100ClassesDataset, prepare_dataloaders_ImageNet100ClassesDataset\n",
    "from src.attacks.attacks import FastGradientSign, ProjectedGradientDescent\n",
    "from src.training.Trainer import Trainer\n",
    "from src.optim.scheduler import CustomScheduler\n",
    "\n",
    "\n",
    "def seed_everything(seed_value=4995):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def load_model(num_classes = 100, model_path = None, to_cuda = True):\n",
    "    if not model_path:\n",
    "        model = torchvision.models.resnet18(pretrained = True)\n",
    "        input_feat = model.fc.in_features\n",
    "        \n",
    "        model.fc = nn.Linear(input_feat, num_classes)\n",
    "        loaded_state_dict = False\n",
    "    \n",
    "    else:\n",
    "        print(\"Loaded\", model_path)\n",
    "        model = torchvision.models.resnet18()\n",
    "        input_feat = model.fc.in_features\n",
    "        model.fc = nn.Linear(input_feat, num_classes)\n",
    "        loaded_model = torch.load(model_path)\n",
    "        model.load_state_dict(loaded_model['model_state_dict'])\n",
    "        loaded_state_dict = True\n",
    "        \n",
    "    if to_cuda:\n",
    "        model = model.to('cuda')\n",
    "        \n",
    "    return model, loaded_state_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eac757a1-e984-4b4b-b2ef-1d06303fa3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded models/resnet_100_imagenet_fine_tuned.pt\n"
     ]
    }
   ],
   "source": [
    "seed_everything()\n",
    "root_dir = 'Data/imagenet100classes'\n",
    "\n",
    "config = {\n",
    "    'lr': 0.001,\n",
    "    'batch_size': 64,\n",
    "    'weight_decay': 0.01\n",
    "}\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\"\"\"\n",
    "resnet, loaded_state_dict = load_model(model_path = 'models/resnet_100_imagenet_adv_training.pt')\n",
    "\n",
    "for name, param in resnet.named_parameters():\n",
    "    if 'fc' not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "train_loader, val_loader = prepare_dataloaders_ImageNet100ClassesDataset(root_dir, batch_size = config['batch_size'])\n",
    "\n",
    "dataloaders = {\n",
    "    'train': train_loader,\n",
    "    'val': val_loader\n",
    "}\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_params = [p for p in resnet.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.Adam(train_params, lr = config['lr'], weight_decay = config['weight_decay'])\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3,5], gamma=0.1)\n",
    "\n",
    "training_params = {\n",
    "    'dataloaders': dataloaders,\n",
    "    'optimizer': optimizer,\n",
    "    'scheduler': scheduler\n",
    "}\n",
    "\n",
    "classes = list(train_loader.dataset.class_name_dict.values())\n",
    "\n",
    "\n",
    "pgd = ProjectedGradientDescent(resnet, loss_fn, iterations = 20, epsilon = 0.25, return_logits=False)\n",
    "\n",
    "\n",
    "trainer = Trainer(resnet, loss_fn, classes, training_params, DEVICE, num_epochs = 1, model_name = 'resnet_100_imagenet_adv_training', save_model = True, model_dir = 'models', adversarial_training = True, adversarial_attack = pgd)\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\"\"\"\n",
    "\n",
    "# Fine-tune all layers\n",
    "resnet, loaded_state_dict = load_model(model_path = 'models/resnet_100_imagenet_fine_tuned.pt')\n",
    "\n",
    "for name, param in resnet.named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "train_loader, val_loader = prepare_dataloaders_ImageNet100ClassesDataset(root_dir, batch_size = config['batch_size'])\n",
    "dataloaders = {\n",
    "    'train': train_loader,\n",
    "    'val': val_loader\n",
    "}\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_params = [p for p in resnet.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.Adam(train_params, lr = 0.0001, weight_decay = config['weight_decay'])\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3,5], gamma=0.1)\n",
    "scheduler = CustomScheduler(optimizer)\n",
    "\n",
    "training_params = {\n",
    "    'dataloaders': dataloaders,\n",
    "    'optimizer': optimizer,\n",
    "    'scheduler': scheduler\n",
    "}\n",
    "\n",
    "classes = list(train_loader.dataset.class_name_dict.values())\n",
    "pgd = ProjectedGradientDescent(resnet, loss_fn, iterations = 5, alpha = 2/255, epsilon = 2/255, return_logits=False)\n",
    "\n",
    "trainer_fine_tune = Trainer(resnet, loss_fn, classes, training_params, DEVICE, num_epochs = 10, model_name = 'resnet_100_imagenet_adv_training_larger_lr_new_scheduler', save_model = True, model_dir = 'models', adversarial_training = True, adversarial_attack = pgd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2bf0d33-afaf-440f-a626-d156cb289974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Epoch: 0\n",
      "Train Loop:\n",
      "Batch: 0 of 2032. Loss: 4.268786907196045. Mean so far: 4.268786907196045. Mean of 100: 4.268786907196045\n",
      "Batch: 100 of 2032. Loss: 1.6217896938323975. Mean so far: 2.0763730695932217. Mean of 100: 2.0544489312171934\n",
      "Batch: 200 of 2032. Loss: 1.5981557369232178. Mean so far: 1.8352575574941303. Mean of 100: 1.591730890274048\n",
      "Batch: 300 of 2032. Loss: 1.1546604633331299. Mean so far: 1.7283950085655795. Mean of 100: 1.5136012852191925\n",
      "Batch: 400 of 2032. Loss: 1.2918837070465088. Mean so far: 1.655310793113233. Mean of 100: 1.4353273046016692\n",
      "Batch: 500 of 2032. Loss: 1.4109734296798706. Mean so far: 1.6087420529234195. Mean of 100: 1.422001404762268\n",
      "Batch: 600 of 2032. Loss: 1.1374326944351196. Mean so far: 1.5781948932395402. Mean of 100: 1.4251536232233049\n",
      "Batch: 700 of 2032. Loss: 1.183595895767212. Mean so far: 1.5544046712839994. Mean of 100: 1.4114254373311996\n",
      "Batch: 800 of 2032. Loss: 1.370367407798767. Mean so far: 1.5354912302616086. Mean of 100: 1.4029080086946488\n",
      "Batch: 900 of 2032. Loss: 1.1503336429595947. Mean so far: 1.5161672202781356. Mean of 100: 1.3613819003105163\n",
      "Batch: 1000 of 2032. Loss: 1.6850683689117432. Mean so far: 1.5076409119230645. Mean of 100: 1.4308188736438752\n",
      "Batch: 1100 of 2032. Loss: 1.3994354009628296. Mean so far: 1.4960486619284108. Mean of 100: 1.380010239481926\n",
      "Batch: 1200 of 2032. Loss: 1.261138677597046. Mean so far: 1.4877062609154021. Mean of 100: 1.3958564257621766\n",
      "Batch: 1300 of 2032. Loss: 1.5162264108657837. Mean so far: 1.4816240460299053. Mean of 100: 1.4085766452550887\n",
      "Batch: 1400 of 2032. Loss: 1.7544972896575928. Mean so far: 1.474304153611199. Mean of 100: 1.3790723532438278\n",
      "Batch: 1500 of 2032. Loss: 1.4083224534988403. Mean so far: 1.4674369962830134. Mean of 100: 1.3712281221151352\n",
      "Batch: 1600 of 2032. Loss: 1.6068243980407715. Mean so far: 1.4629038940140189. Mean of 100: 1.3948620289564133\n",
      "Batch: 1700 of 2032. Loss: 1.4049769639968872. Mean so far: 1.4591865623999456. Mean of 100: 1.399672083258629\n",
      "Batch: 1800 of 2032. Loss: 1.4552468061447144. Mean so far: 1.4550757601418673. Mean of 100: 1.3851510137319565\n",
      "Batch: 1900 of 2032. Loss: 1.3738305568695068. Mean so far: 1.451343103467258. Mean of 100: 1.3841179567575455\n",
      "Batch: 2000 of 2032. Loss: 1.1699466705322266. Mean so far: 1.4475118077021727. Mean of 100: 1.374678875207901\n",
      "Train loss: 1.448255855883435. Train measure: 0.5940538461538462\n",
      "Train loop took 6309.11580324173\n",
      "Val Loop:\n",
      "Batch: 0 of 79. Loss: 0.8240412473678589. Mean so far: 0.8240412473678589. Mean of 100: 0.8240412473678589\n",
      "Validation loss: 1.6207028069073641. Val measure: 0.5624\n",
      "Validation loop took 228.47864317893982\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CustomScheduler' object has no attribute 'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18971/352744195.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer_fine_tune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Losses:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainer_fine_tune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracies:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainer_fine_tune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dlnn-project/src/training/Trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m                         torch.save({\n\u001b[1;32m    115\u001b[0m                             \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcurrent_epoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                             \u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                             \u001b[0;34m'optimizer_state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                             \u001b[0;34m'scheduler_state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CustomScheduler' object has no attribute 'state_dict'"
     ]
    }
   ],
   "source": [
    "trainer_fine_tune.train()\n",
    "\n",
    "\n",
    "print(\"Losses:\",trainer_fine_tune.epoch_loss)\n",
    "print(\"Accuracies:\",trainer_fine_tune.epoch_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930186d5-848b-4a95-aa6f-f92381a1665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "continue_dict = {\n",
    "            \"best_measure\": 0.5624,\n",
    "            \"train_measure_at_best\": 0.5940538461538462,\n",
    "            \"train_loss_at_best\":  1.448255855883435,\n",
    "            \"best_eval_loss\": 1.6207028069073641,\n",
    "            \"best_epoch\": 0,\n",
    "            \"latest_epoch\": 0,\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
